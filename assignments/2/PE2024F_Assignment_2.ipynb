{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0WCcr_F4-oP"
   },
   "source": [
    "# Topic 2: LangChain & Prompt Engineering\n",
    "\n",
    "**Learning Objectives:**\n",
    "\n",
    "* Learn how to use LangChain pipeline.\n",
    "* Struct LLM output for prompt chaining.\n",
    "* Chain-of-Thought prompting.\n",
    "* Use CoT prompting for various tasks.\n",
    "\n",
    "**Outline:**\n",
    "\n",
    "1. **LangChain**\n",
    "2. **Prompt Chaining**\n",
    "3. **Chain-of-Thought**\n",
    "4. **Assignment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NOFruKHs76Az"
   },
   "source": [
    "## 1. LangChain\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "*   **Langchain module**: Including chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "*   **LangGraph**: Build multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.\n",
    "*   **LangServe**: Deploy LangChain chains as REST APIs.\n",
    "*   **LangSmith**: A developer platform that lets developers debug, test, evaluate, and monitor LLM applications.\n",
    "\n",
    "You can check the LangChain docs for more information. ([link](https://python.langchain.com/docs/introduction/))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9o8gGNgEx12"
   },
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YD-GUB6s78er"
   },
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "!pip install -qU langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ko0_OeTDlr3"
   },
   "outputs": [],
   "source": [
    "# Install langchain-openai\n",
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2duN-x-D2tMq"
   },
   "source": [
    "**Set API key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZqZkGINv-9M"
   },
   "outputs": [],
   "source": [
    "# Set API key\n",
    "OPENAI_API_KEY=\"YOUR_API_KEY_HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztDA7lWl5Z4s"
   },
   "source": [
    "### Using Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Qxvsp02nW5y"
   },
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkLY_KHgGkLV"
   },
   "source": [
    "Can use the model directly by passing messages to the `.invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2QXV7y2GMcP"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Korean\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9IxRzYIzG2dG"
   },
   "source": [
    "### OutputParsers\n",
    "Response from the model contains string response and metadata about the response.\n",
    "LangChain output parser can parse out the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QQWu7aSbG2FR"
   },
   "outputs": [],
   "source": [
    "# Import simple output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2aGFN_SC-4u2"
   },
   "outputs": [],
   "source": [
    "# Pass the response to the parser\n",
    "result = model.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urDq3ouyRnKy"
   },
   "source": [
    "### Prompt Templates\n",
    "PromptTemplates are a concept in LangChain designed to assist with the transformation\n",
    " from the raw user input to a list of messages ready to pass to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaicVrsZR_aa"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")\n",
    "\n",
    "# Pass the dictionary input to the prompt template\n",
    "result = prompt_template.invoke({\"language\": \"English\", \"text\": \"좋은 하루 되세요!\"})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sp2giMNeHje3"
   },
   "source": [
    "### Chaining\n",
    "LangChain can \"chain\" the model and the output parser. The `|` operator is used in LangChain to combine elements together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9VeNQ1Y_AnB"
   },
   "outputs": [],
   "source": [
    "# Create chain\n",
    "chain = model | parser\n",
    "\n",
    "# Use chain\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following into English:\"),\n",
    "    HumanMessage(content=\"좋은 하루 되세요!\"),\n",
    "]\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAbzjnysS1pa"
   },
   "source": [
    "We can combine with the prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N92ZQTfL6gUJ"
   },
   "outputs": [],
   "source": [
    "chain = prompt_template | model | parser\n",
    "\n",
    "chain.invoke({\"language\": \"English\", \"text\": \"좋은 하루 되세요!\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qK3ajcIzVTVg"
   },
   "source": [
    "LCEL is a declarative way to chain LangChain components.\n",
    "![LCEL](https://raw.githubusercontent.com/GSDSAML/prompt-engineering-2024fall/main/lcel.png)\n",
    "\n",
    "The input type and output type varies by component:\n",
    "\n",
    "| Component     | Input Type                                      | Output Type                    |\n",
    "|---------------|-------------------------------------------------|---------------------------------|\n",
    "| Prompt        | Dictionary                                       | PromptValue                    |\n",
    "| ChatModel     | Single string, list of chat messages or a PromptValue | ChatMessage                    |\n",
    "| LLM           | Single string, list of chat messages or a PromptValue | String                         |\n",
    "| OutputParser  | The output of an LLM or ChatModel                | Depends on the parser           |\n",
    "| Retriever     | Single string                                    | List of Documents               |\n",
    "| Tool          | Single string or dictionary, depending on the tool | Depends on the tool             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MAdbQUYYrJ2"
   },
   "source": [
    "## 2. Prompt Chaining\n",
    "To improve the reliability and performance of LLMs, complex tasks should be split into its subtasks. The LLM is prompted with a subtask and then its response is used as input to another prompt.\n",
    "\n",
    "With prompt chaining, you can debug complex LLM tasks with the generated outputs of subtasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uv6c_VS22KmI"
   },
   "source": [
    "This example uses two LLMs for generate a topic and blog post title related to the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU64B17R2K5x"
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt template for the first step: Generate a topic\n",
    "topic_template = PromptTemplate(\n",
    "    input_variables=[\"domain\"],\n",
    "    template=\"Generate a topic related to {domain}.\",\n",
    ")\n",
    "\n",
    "# Prompt template for the second step: Generate a blog post title\n",
    "title_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate a blog post title about {topic}.\",\n",
    ")\n",
    "\n",
    "# Chain the prompts together\n",
    "chain = topic_template | model | parser | title_template | model | parser\n",
    "\n",
    "# Run the chain\n",
    "result = chain.invoke({\"domain\": \"artificial intelligence\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKSz_Hoj9GPy"
   },
   "outputs": [],
   "source": [
    "# Compare with the single LLM\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"domain\"],\n",
    "    template=\"Generate a topic related to {domain}. Then, generate a blog post title about the topic you generated.\"\n",
    ")\n",
    "\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "result = chain.invoke({\"domain\": \"artificial intelligence\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqeK0fIX7zk0"
   },
   "source": [
    "Another example is to write a short synopsis of the story for the genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6u42GMKh3FLz"
   },
   "outputs": [],
   "source": [
    "# Prompt template for the first step: Generate a creative story idea\n",
    "story_idea_template = PromptTemplate(\n",
    "    input_variables=[\"genre\"],\n",
    "    template=\"Generate a creative story idea within the genre of {genre}.\",\n",
    ")\n",
    "\n",
    "# Prompt template for the second step: Develop the story's main characters\n",
    "characters_template = PromptTemplate(\n",
    "    input_variables=[\"story_idea\"],\n",
    "    template=\"Based on the story idea: {story_idea}, develop 3 main characters with distinct personalities and backstories.\",\n",
    ")\n",
    "\n",
    "# Prompt template for the third step: Write a short synopsis of the story\n",
    "synopsis_template = PromptTemplate(\n",
    "    input_variables=[\"story_idea\", \"characters\"],\n",
    "    template=\"Given the story idea: {story_idea} and the characters: {characters}, write a short synopsis of the story, highlighting the plot and conflict.\",\n",
    ")\n",
    "\n",
    "# Chain the prompts\n",
    "chain1 = (\n",
    "    story_idea_template\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain2 = (\n",
    "    characters_template\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "chain3 = (\n",
    "    synopsis_template\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "story_idea = chain1.invoke({\"genre\": \"science fiction\"})\n",
    "characters = chain2.invoke({\"story_idea\": story_idea})\n",
    "result = chain3.invoke({\"story_idea\": story_idea, \"characters\": characters})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZBl4-eR9dpf"
   },
   "outputs": [],
   "source": [
    "# Single LLM version\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"genre\"],\n",
    "    template=\"\"\"Generate a creative story idea within the genre of {genre}.\n",
    "    Based on the story idea, develop 3 main characters with distinct personalities and backstories.\n",
    "    Given the story idea and the characters, write a short synopsis of the story, highlighting the plot and conflict.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "result = chain.invoke({\"genre\": \"science fiction\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xTRtC4H9dJa"
   },
   "source": [
    "We can use RunnableParallel to execute multiple Runnables in parallel, and to return the output of these Runnables as a map.\n",
    "\n",
    "The example below uses RunnableParallel to make pros and cons for a topic and merge them into a single paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPQiDB148EsX"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Prompt template for generating pros\n",
    "pros_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate 3 pros for the topic: {topic}.\",\n",
    ")\n",
    "\n",
    "# Prompt template for generating cons\n",
    "cons_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Generate 3 cons for the topic: {topic}.\",\n",
    ")\n",
    "\n",
    "# Create chains for pros and cons\n",
    "pros_chain = pros_template | model | parser\n",
    "cons_chain = cons_template | model | parser\n",
    "\n",
    "# Combine the two chains in parallel\n",
    "combined_chain = RunnableParallel(pros=pros_chain, cons=cons_chain)\n",
    "\n",
    "# Create a chain to merge the results\n",
    "merge_chain = (\n",
    "    PromptTemplate(\n",
    "        input_variables=[\"pros\", \"cons\"],\n",
    "        template=\"\"\"\n",
    "        Here are the pros: {pros}\n",
    "        Here are the cons: {cons}\n",
    "        Merge the pros and cons into a single paragraph summarizing the topic.\n",
    "        \"\"\"\n",
    "    )\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "\n",
    "# Combine the parallel chain with the merge chain\n",
    "full_chain = combined_chain | merge_chain\n",
    "\n",
    "# Run the chain\n",
    "result = full_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WepdIbNBpDS"
   },
   "source": [
    "## 3. Chain-of-Thought\n",
    "\n",
    "Chain-of-Thought prompting let the LLM to construct an entire logical argument, including premises and a conclusion.\n",
    "\n",
    "From GPT-4, LLM will generate Chain-of-Thought by default. So we will compare CoT with \"do not explain\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nI9UZfc_B3iI"
   },
   "outputs": [],
   "source": [
    "cot_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "no_cot_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"\n",
    "    Question: {question}\n",
    "    Do not explain, just answer.\n",
    "    Answer:\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "cot_chain = cot_prompt_template | model | parser\n",
    "no_cot_chain = no_cot_prompt_template | model | parser\n",
    "\n",
    "\n",
    "question = \"Darrell and Allen's ages are in the ratio of 7:11. If their total age now is 162, calculate Allen's age 10 years from now.\"\n",
    "\n",
    "cot_result = cot_chain.invoke({\"question\": question})\n",
    "no_cot_result = no_cot_chain.invoke({\"question\": question})\n",
    "\n",
    "print(f\"Chain-of-Thought Result:\\n{cot_result}\\n\")\n",
    "print(f\"No Chain-of-Thought Result:\\n{no_cot_result}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxwFgTQ7gtbZ"
   },
   "source": [
    " CoT can combine with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71CSVVQCCZqU"
   },
   "outputs": [],
   "source": [
    "# Provide the cipher puzzle without examples\n",
    "cipher_puzzle_template = PromptTemplate(\n",
    "    input_variables=[\"cipher_text\"],\n",
    "    template=\"\"\"\n",
    "    Decode following text:\n",
    "    {cipher_text}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "cipher_chain = cipher_puzzle_template | model | parser\n",
    "\n",
    "result = cipher_chain.invoke({\"cipher_text\": \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7nQD-gag5Xh"
   },
   "outputs": [],
   "source": [
    "# Provide the cipher puzzle with example, without reasoning\n",
    "cipher_puzzle_template = PromptTemplate(\n",
    "    input_variables=[\"cipher_text\"],\n",
    "    template=\"\"\"\n",
    "    oyfjdnisdr rtqwainr acxz mynzbhhx -> Think step by step\n",
    "\n",
    "    Use the example above to decode:\n",
    "    {cipher_text}\n",
    "    \"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "cipher_chain = cipher_puzzle_template | model | parser\n",
    "\n",
    "result = cipher_chain.invoke({\"cipher_text\": \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nba-MVmQlUl0"
   },
   "source": [
    "### Exercise: Try to make reasoning for few-shot example on your own  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3U6mxG1itRR"
   },
   "outputs": [],
   "source": [
    "# Provide the cipher puzzle with example and reasoning\n",
    "\n",
    "cipher_puzzle_template = PromptTemplate(\n",
    "    input_variables=[\"cipher_text\"],\n",
    "    ########### Modify here #############\n",
    "    template=\"\"\"\n",
    "    Decode following text:\n",
    "\n",
    "    Example 1:\n",
    "    Cipher text: oyfjdnisdr rtqwainr acxz mynzbhhx\n",
    "    Reasoning:\n",
    "\n",
    "    Cipher text: {cipher_text}\n",
    "    Reasoning:\n",
    "    \"\"\",\n",
    "    #####################################\n",
    ")\n",
    "\n",
    "\n",
    "cipher_chain = cipher_puzzle_template | model | parser\n",
    "\n",
    "result = cipher_chain.invoke({\"cipher_text\": \"oyekaijzdf aaptcg suaokybhai ouow aqht mynznvaatzacdfoulxxz\"})\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb_SM5lqlhdB"
   },
   "source": [
    "<details>\n",
    "<summary>Click here for the sample prompt!</summary>\n",
    "\n",
    "```\n",
    "Break down the ciphertext into pairs:\n",
    "    1. First word: \"oyfjdnisdr\"\n",
    "      a. Pairs: oy, fj, dn, is, dr\n",
    "      b. Decoded letters:\n",
    "        oy → (15+25)/2 = 20 → T\n",
    "        fj → (6+10)/2 = 8 → H\n",
    "        dn → (4+14)/2 = 9 → I\n",
    "        is → (9+19)/2 = 14 → N\n",
    "        dr → (4+18)/2 = 11 → K\n",
    "      c. Decoded word: THINK\n",
    "    2. Second word: \"rtqwainr\"\n",
    "      a. Paris: rt, qw, ai, nr\n",
    "      b. Decoded letters:\n",
    "        rt → S\n",
    "        qw → T\n",
    "        ai → E\n",
    "        nr → P\n",
    "      c. Decoded word: STEP\n",
    "    3. Third word: \"acxz\"\n",
    "      a. Pairs: ac, xz\n",
    "      b. Decoded letters:\n",
    "        ac → B\n",
    "        xz → Y\n",
    "      c. Decoded word: BY\n",
    "    4. Fourth word: \"mynzbhhx\"\n",
    "      a. Pairs: my, nz, bh, hx\n",
    "      b. Decoded letters:\n",
    "        my → S\n",
    "        nz → T\n",
    "        bh → E\n",
    "        hx → P\n",
    "      c. Decoded word: STEP\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bICWWJCPGQ_6"
   },
   "source": [
    "# Assignment 2: Python Code Generation\n",
    "\n",
    "For the given tasks, use Chain-of-thought prompting to guide the LLM in generating Python code.\n",
    "\n",
    "The code **must include test code** with **at least three examples** to verify that functions works correctly.\n",
    "\n",
    "You only need to complete **4 tasks out of the total 6 tasks**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BpEFVl8vXL-"
   },
   "outputs": [],
   "source": [
    "# Install langchain\n",
    "!pip install -qU langchain\n",
    "\n",
    "# Install langchain-openai\n",
    "!pip install -qU langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEkjKbQEvf38"
   },
   "outputs": [],
   "source": [
    "# Set API key\n",
    "OPENAI_API_KEY=\"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Prepare model\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0, api_key=OPENAI_API_KEY)\n",
    "\n",
    "# Import simple output parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkHN7hAQt-iA"
   },
   "source": [
    "## Example Task 1: Palindrome Checker\n",
    "\n",
    "Write a Python function `is_palindrome(str)` that checks a given string `str` is a palindrome. Ignore spaces, punctuation, and case sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xSZ66u8gt-C-"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "A Python function is_palindrome(s) that checks whether a given string str is a palindrome. Ignore spaces, punctuation, and case sensitivity.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "You should define the function first.\n",
    "In the function, normalize the input string. and compare with the reverse string.\n",
    "After defining the function, you should write a test code with at least three examples.\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ek2tF_Qd3-M"
   },
   "source": [
    "And paste the code to the cell below and run it for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FKBAMvB5r-M"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################\n",
    "import string\n",
    "\n",
    "def is_palindrome(s):\n",
    "    # Normalize the string: remove punctuation, spaces, and convert to lowercase\n",
    "    normalized_str = ''.join(char.lower() for char in s if char.isalnum())\n",
    "\n",
    "    # Check if the normalized string is equal to its reverse\n",
    "    return normalized_str == normalized_str[::-1]\n",
    "\n",
    "# Test cases\n",
    "if __name__ == \"__main__\":\n",
    "    test_cases = [\n",
    "        \"A man, a plan, a canal, Panama!\",\n",
    "        \"Was it a car or a cat I saw?\",\n",
    "        \"No 'x' in Nixon\",\n",
    "        \"Hello, World!\"\n",
    "    ]\n",
    "\n",
    "    for test in test_cases:\n",
    "        result = is_palindrome(test)\n",
    "        print(f\"'{test}' -> {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcNvJ36lu-WW"
   },
   "source": [
    "## Example Task 2: Prime Numbers in a Range\n",
    "\n",
    "Create a Python function `find_primes(a, b)` that returns a list of all prime numbers between two integers `a` and `b` (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Lf5gu3Fuf6H"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "A Python function find_primes(a, b) that returns a list of all prime numbers between two integers a and b (inclusive).\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "First, you should define a function is_prime() to check whether the given number is a prime number or not.\n",
    "And then, define a find_primes() function that finds all prime numbers between given two integers(inclusive).\n",
    "After defining functions, you should write a test code with at least three examples.\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaH-biBfumyg"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################\n",
    "def is_prime(n):\n",
    "    \"\"\"Check if a number is prime.\"\"\"\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_primes(a, b):\n",
    "    \"\"\"Return a list of all prime numbers between two integers a and b (inclusive).\"\"\"\n",
    "    primes = []\n",
    "    for num in range(a, b + 1):\n",
    "        if is_prime(num):\n",
    "            primes.append(num)\n",
    "    return primes\n",
    "\n",
    "# Test code\n",
    "if __name__ == \"__main__\":\n",
    "    # Test case 1: Primes between 10 and 30\n",
    "    print(\"Primes between 10 and 30:\", find_primes(10, 30))\n",
    "\n",
    "    # Test case 2: Primes between 1 and 20\n",
    "    print(\"Primes between 1 and 20:\", find_primes(1, 20))\n",
    "\n",
    "    # Test case 3: Primes between 50 and 60\n",
    "    print(\"Primes between 50 and 60:\", find_primes(50, 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMuzhjvEv9YM"
   },
   "source": [
    "## Task 1: Anagram Grouping\n",
    "\n",
    "Write a Python function `group_anagrams(words)` that takes a list of strings and groups them into anagrams.\n",
    "\n",
    "Return a list of lists, where each sublist contains `words` that are anagrams of each other.\n",
    "\n",
    "Ex) For the input `[\"cat\", \"act\", \"not\", \"ate\", \"ton\", \"tea\"]`, the output will be:\n",
    "```\n",
    "[['cat', 'act'], ['not', 'ton'], ['ate', 'tea]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRAFz6YZwDBK"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "A Python function group_anagrams(words) that takes a list of strings and groups them into anagrams.\n",
    "Return a list of lists, where each sublist contains words that are anagrams of each other.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1wnKHIZgBW_"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuZyDXPJGmRf"
   },
   "source": [
    "## Task 2: Word Ladder Length\n",
    "\n",
    "Given two words, `beginWord` and `endWord`, and a dictionary `wordList`, write a Python function `ladder_length(beginWord, endWord, wordList)` that returns the length of the shortest transformation sequence and the path from `beginWord` to `endWord`.\n",
    "\n",
    "Each transformed word must exist in `wordList`, and only one letter can be changed at a time.\n",
    "\n",
    "If no such sequence exists, return 0.\n",
    "\n",
    "Ex) Input: wordList = `{ABCD, EBAD, EBCD, XYZA}`, beginWord = `ABCV`, endWord = `EBAD`\n",
    "\n",
    "Output: Length is 4, the path is `ABCV – ABCD – EBCD – EBAD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCtjmHe1Hj1v"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "Given two words and a dictionary, write a Python function ladder_length() that returns the length of the shortest transformation sequence and the path from begin word to end word.\n",
    "Each transformed word must exist in the dictionary, and only one letter can be changed at a time.\n",
    "If no such sequence exists, return 0.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoQ2sqNUHuJO"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcLenIu1-MSQ"
   },
   "source": [
    "## Task 3: 24 Game solver\n",
    "\n",
    "24 game is to make 24 with 4 numbers and elementary arithmetic operations (+ - × and /).\n",
    "\n",
    "Please make a Python code for solving 24 game with given 4 numbers.\n",
    "\n",
    "If there is no solution, the output should be \"no solution found.\"\n",
    "\n",
    "Ex) Input: `[4, 7, 8, 8]`\n",
    "\n",
    "Output: `(7-(8/8))*4=24`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apJArcAH-Xfu"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "24 game is to make 24 with exact 4 numbers and elementary arithmetic operations (+ - × and /).\n",
    "Please make a Python code for solve 24 game with given 4 numbers.\n",
    "If there is no solution, the output should be \"no solution found.\"\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUTT_5mpAlWu"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vdt1C-N15O7B"
   },
   "source": [
    "## Task 4: Word Chain\n",
    "\n",
    "Generate a python function find_word_chain() that finds longest word chain for the given list of words.\n",
    "\n",
    "In a word chain, next words begin with the letter that the previous word ended with.\n",
    "\n",
    "Ex) Input: `[banana, apple, cat, math, dog, exam]`\n",
    "\n",
    "Output: `banana -> apple -> exam -> math`: word chain with length 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB6sltTp5PVj"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "A python function find_word_chain() that finds longest word chain for the given list of words.\n",
    "In a word chain, next words begin with the letter that the previous word ended with.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9JJTAWGIlPsu"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vq5EtEb3w2rF"
   },
   "source": [
    "## Task 5: Gram Matrix Calculation\n",
    "\n",
    "Write a Python code that calculate a Gram matrix for the given matrix.\n",
    "\n",
    "Gram matrix G for the given matrix A can be calculated by multiplying transpose of A with the matrix A. [ref](https://en.wikipedia.org/wiki/Gram_matrix)\n",
    "\n",
    "G = AᵀA\n",
    "\n",
    "Ex) Input: [[1,2],[3,4],[5,6]]\n",
    "\n",
    "Output: [[35, 44], [44, 56]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRjoiF7jxF_j"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "Calculate a Gram matrix for the given matrix.\n",
    "Gram matrix G for the given matrix A can be calculated by multiplying transpose of A with the matrix A.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nMulML3xJTe"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSl35QEWU7nU"
   },
   "source": [
    "## Task 6: Hanoi Tower Variation (Hard)\n",
    "\n",
    "Generate Python function hanoi_solver() that solves Hanoi tower problem, but discs can only move to the adjacent rods.\n",
    "\n",
    "Ex) Input: 3 discs\n",
    "\n",
    "Output:\n",
    "```\n",
    "1 A->B\n",
    "1 B->C\n",
    "2 A->B\n",
    "1 C->B\n",
    "1 B->A\n",
    "2 B->C\n",
    "1 A->B\n",
    "1 B->C\n",
    "3 A->B\n",
    "1 C->B\n",
    "1 B->A\n",
    "2 C->B\n",
    "1 A->B\n",
    "1 B->C\n",
    "2 B->A\n",
    "1 C->B\n",
    "1 B->A\n",
    "3 B->C\n",
    "1 A->B\n",
    "1 B->C\n",
    "2 A->B\n",
    "1 C->B\n",
    "1 B->A\n",
    "2 B->C\n",
    "1 A->B\n",
    "1 B->C\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bs5InbwuV-mI"
   },
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"problem_statement\", \"cot\"],\n",
    "    template=\"\"\"\n",
    "    Generate a python code for the following problem statement:\n",
    "\n",
    "    {problem_statement}\n",
    "\n",
    "    {cot}\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "\n",
    "problem_statement = \"\"\"\n",
    "Generate Python function hanoi_solver() that solves Hanoi tower problem, but discs can only move to the adjacent rods.\n",
    "\"\"\"\n",
    "\n",
    "################ TODO: Modify here ################\n",
    "\n",
    "cot = \"\"\"\n",
    "\"\"\"\n",
    "\n",
    "###################################################\n",
    "\n",
    "result = chain.invoke({\"problem_statement\": problem_statement, \"cot\": cot})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS9IyANDns3j"
   },
   "outputs": [],
   "source": [
    "################ TODO: Paste the code here ################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLmAONhMM6CM9DFTjXQmYq",
   "provenance": [
    {
     "file_id": "1XZvYCLVn0lC6zP02lDdQS5mgtzhf_JNG",
     "timestamp": 1727014606026
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
